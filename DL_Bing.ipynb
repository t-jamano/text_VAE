{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling1D, merge, Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from scipy import spatial\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import csv\n",
    "import os\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.16 s, sys: 504 ms, total: 7.66 s\n",
      "Wall time: 7.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "TRAIN_DATA_FILE = '/data/chzho/deepqts/train_data/unifiedclick/join_oneyearsample_2B_training_all_top10'\n",
    "num_read_row = 1000000\n",
    "df = pd.read_csv(TRAIN_DATA_FILE, sep=\"\\t\", usecols=[0,1,3], names=['label', 'q', 'd'], header=None , error_bad_lines=False, nrows=num_read_row)\n",
    "df = df.dropna()\n",
    "\n",
    "TEST_DATA_FILE = '/data/chzho/deepqts/test_data/uhrs/unified/uhrs_do_10'\n",
    "df_qd = pd.read_csv(TEST_DATA_FILE, sep=\"\\t\", usecols=[0,1,3,5], names=['label', 'q', 'd', 'market'], header=None , error_bad_lines=False)\n",
    "df_qd = df_qd.dropna()\n",
    "df_qd = df_qd[df_qd.market == \"en-US\"]\n",
    "\n",
    "TEST_DATA_FILE = '/data/chzho/deepqts/test_data/julyflower/julyflower_original.tsv'\n",
    "df_qq = pd.read_csv(TEST_DATA_FILE, sep=\"\\t\", names=['q', 'd', 'label'], header=None , error_bad_lines=False)\n",
    "df_qq = df_qq.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = df_qd.q.tolist() + df_qd.d.tolist() + df_qq.q.tolist() + df_qq.d.tolist() + df.q.tolist() + df.d.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 711013 unique tokens\n",
      "Number of Vocab: 500001\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "MAX_SEQUENCE_LENGTH = 5\n",
    "\n",
    "tokenizer = Tokenizer(500000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index #the dict values start from 1 so this is fine with zeropadding\n",
    "index2word = {v: k for k, v in word_index.items()}\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "NB_WORDS = (min(tokenizer.nb_words, len(word_index)) + 1 ) #+1 for zero padding\n",
    "print('Number of Vocab: %d' % NB_WORDS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_train_qq = tokenizer.texts_to_sequences(df_qq.q.tolist())\n",
    "q_train_qq = pad_sequences(q_train_qq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "d_train_qq = tokenizer.texts_to_sequences(df_qq.d.tolist())\n",
    "d_train_qq = pad_sequences(d_train_qq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y_train_qq = df_qq.label.values\n",
    "\n",
    "uns_q = pad_sequences(tokenizer.texts_to_sequences(df.q.tolist()), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "uns_d = pad_sequences(tokenizer.texts_to_sequences(df.d.tolist()), maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "GLOVE_EMBEDDING = '/home/t-jamano/data/glove/glove.6B.50d.txt'\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_EMBEDDING, encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < NB_WORDS:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be the word embedding of 'unk'.\n",
    "            glove_embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            glove_embedding_matrix[i] = embeddings_index.get('unk')\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class W2V():\n",
    "    def __init__(self, input_dim, emb_dim, nb_words, weights=None):\n",
    "\n",
    "        q_input = Input(shape=(input_dim,))\n",
    "        d_input = Input(shape=(input_dim,))\n",
    "        if weights != None:\n",
    "            emb = Embedding(nb_words, emb_dim, input_length=input_dim, weights=[weights])\n",
    "        else:\n",
    "            emb = Embedding(nb_words, emb_dim, input_length=input_dim)\n",
    "\n",
    "        \n",
    "        q_embed = GlobalAveragePooling1D()(emb(q_input))\n",
    "        d_embed = GlobalAveragePooling1D()(emb(d_input))\n",
    "\n",
    "        concat = merge([q_embed, d_embed], mode=\"concat\")\n",
    "\n",
    "\n",
    "        pred = Dense(1, activation='sigmoid')(concat)\n",
    "\n",
    "        self.model = Model(input=[q_input, d_input], output=pred)\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v = W2V(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, NB_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 213 samples, validate on 213 samples\n",
      "Epoch 1/10\n",
      "0s - loss: 0.6932 - acc: 0.5305 - val_loss: 0.6916 - val_acc: 0.5399\n",
      "Epoch 2/10\n",
      "0s - loss: 0.6845 - acc: 0.7089 - val_loss: 0.6904 - val_acc: 0.5493\n",
      "Epoch 3/10\n",
      "0s - loss: 0.6754 - acc: 0.7606 - val_loss: 0.6892 - val_acc: 0.5728\n",
      "Epoch 4/10\n",
      "0s - loss: 0.6658 - acc: 0.8028 - val_loss: 0.6879 - val_acc: 0.5775\n",
      "Epoch 5/10\n",
      "0s - loss: 0.6554 - acc: 0.8357 - val_loss: 0.6865 - val_acc: 0.5775\n",
      "Epoch 6/10\n",
      "0s - loss: 0.6433 - acc: 0.8498 - val_loss: 0.6849 - val_acc: 0.5775\n",
      "Epoch 7/10\n",
      "0s - loss: 0.6303 - acc: 0.8592 - val_loss: 0.6834 - val_acc: 0.5775\n",
      "Epoch 8/10\n",
      "0s - loss: 0.6153 - acc: 0.8779 - val_loss: 0.6817 - val_acc: 0.5681\n",
      "Epoch 9/10\n",
      "0s - loss: 0.5995 - acc: 0.8873 - val_loss: 0.6801 - val_acc: 0.5681\n",
      "Epoch 10/10\n",
      "0s - loss: 0.5816 - acc: 0.8967 - val_loss: 0.6784 - val_acc: 0.5822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0df72f25f8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.model.fit([q_train_qq, d_train_qq], y_train_qq, verbose=2, batch_size=32, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = w2v.model.predict([q_train_qq[213:], d_train_qq[213:]])\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train_qq[213:], pred, pos_label=1)\n",
    "auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
